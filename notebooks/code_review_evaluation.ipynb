{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Review Tool Evaluation\n",
    "\n",
    "This notebook runs W&B Weave evaluations for the code review tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "PROJECT_NAME = \"bugbug-code-review-eval\"\n",
    "\n",
    "_ = weave.init(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = weave.ref(\"code_review_eval_legacy\").get()\n",
    "\n",
    "print(f\"Dataset has {len(dataset.rows)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cached_property\n",
    "\n",
    "from bugbug.tools.code_review.agent import CodeReviewTool\n",
    "from bugbug.tools.core.platforms.phabricator import PhabricatorPatch\n",
    "\n",
    "\n",
    "class CodeReviewModel(weave.Model):\n",
    "    \"\"\"Weave Model wrapper for CodeReviewTool.\"\"\"\n",
    "\n",
    "    @cached_property\n",
    "    def tool(self):\n",
    "        return CodeReviewTool.create()\n",
    "\n",
    "    @weave.op()\n",
    "    def invoke(self, diff_id: int, patch_summary: str) -> dict:\n",
    "        patch = PhabricatorPatch(diff_id=diff_id)\n",
    "        comments = self.tool.generate_review_comments(patch, patch_summary)\n",
    "        return {\n",
    "            \"comments\": comments,\n",
    "        }\n",
    "\n",
    "\n",
    "model = CodeReviewModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bugbug.tools.code_review.scorer import BasicMetricsScorer, LLMCommentMatchingScorer\n",
    "\n",
    "evaluation = weave.Evaluation(\n",
    "    dataset=dataset,\n",
    "    scorers=[BasicMetricsScorer(), LLMCommentMatchingScorer()],\n",
    ")\n",
    "\n",
    "results = await evaluation.evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract key metrics for visualization\n",
    "metrics = {\n",
    "    \"Recall (Valid)\": results.get(\"LLMCommentMatchingScorer\", {}).get(\n",
    "        \"recall_valid\", 0\n",
    "    ),\n",
    "    \"Recall (Invalid)\": results.get(\"LLMCommentMatchingScorer\", {}).get(\n",
    "        \"recall_invalid\", 0\n",
    "    ),\n",
    "    \"Missed Valid Rate\": results.get(\"LLMCommentMatchingScorer\", {}).get(\n",
    "        \"missed_valid_rate\", 0\n",
    "    ),\n",
    "    \"Error Rate\": results.get(\"BasicMetricsScorer\", {}).get(\"error_rate\", 0),\n",
    "}\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(\n",
    "    metrics.keys(), metrics.values(), color=[\"green\", \"orange\", \"red\", \"gray\"]\n",
    ")\n",
    "ax.set_ylabel(\"Rate\")\n",
    "ax.set_title(\"Code Review Evaluation Metrics\")\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, metrics.values()):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.02,\n",
    "        f\"{value:.2%}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment counts comparison\n",
    "basic_metrics = results.get(\"BasicMetricsScorer\", {})\n",
    "\n",
    "counts = {\n",
    "    \"Generated\": basic_metrics.get(\"total_generated_comments\", 0),\n",
    "    \"Ground Truth (Valid)\": basic_metrics.get(\"total_ground_truth_valid\", 0),\n",
    "    \"Ground Truth (Invalid)\": basic_metrics.get(\"total_ground_truth_invalid\", 0),\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars = ax.bar(counts.keys(), counts.values(), color=[\"blue\", \"green\", \"red\"])\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Comment Counts\")\n",
    "\n",
    "for bar, value in zip(bars, counts.values()):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.5,\n",
    "        str(int(value)),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View in W&B\n",
    "\n",
    "Visit [W&B Weave](https://wandb.ai) to see detailed traces, compare evaluations, and explore individual predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bugbug (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
