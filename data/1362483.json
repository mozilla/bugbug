{"cf_tracking_thunderbird_esr52": "---", "cf_tracking_firefox60": "---", "cf_tracking_firefox61": "---", "cf_tracking_firefox62": "---", "mentors_detail": [], "depends_on": [], "cf_tracking_seamonkey259": "---", "cf_tracking_seamonkey258": "---", "is_open": true, "keywords": ["perf", "regression", "regressionwindow-wanted"], "cf_tracking_seamonkey253": "---", "cf_last_resolved": null, "votes": 1, "cf_status_seamonkey253": "---", "is_confirmed": true, "is_creator_accessible": true, "cf_status_seamonkey258": "---", "cf_status_seamonkey259": "---", "cf_status_thunderbird_52": "---", "platform": "Unspecified", "cf_crash_signature": "", "cf_status_thunderbird_esr52": "---", "cf_status_thunderbird_56": "---", "cf_status_thunderbird_57": "---", "cf_status_thunderbird_54": "---", "cf_status_thunderbird_58": "---", "cf_status_thunderbird_59": "---", "cf_tracking_firefox_esr60": "---", "cf_tracking_thunderbird_esr60": "---", "cf_status_seamonkey249": "---", "summary": "Gloda stuck in a loop with high CPU and error console gloda.index_msg WARN \"Observed header that claims to be gloda indexed but that gloda has never heard of during compaction\". Folder repair moves the issue to a different folder.", "cf_status_seamonkey257esr": "---", "cf_tracking_seamonkey257esr": "---", "cf_tracking_thunderbird_62": "---", "attachments": [], "cf_tracking_thunderbird_60": "---", "cf_tracking_thunderbird_61": "---", "creation_time": "2017-05-05T17:58:36Z", "cf_tracking_firefox_relnote": "---", "assigned_to_detail": {"email": "nobody@mozilla.org", "id": 1, "name": "nobody@mozilla.org", "real_name": "Nobody; OK to take it and work on it"}, "comments": [{"text": "Noticed higher than usual cpu activity and traced it back to gloda. It's spamming messages like that in hundreds per second.\n\nThe message mentioned is present in the folder and has been for 2 years already. If I repair said folder it moves on to another one and claims there's also a message with sketchy key there. Tried repairing a few folders and it got back to initial folder only to claim another message key sketchy.\n\nSo, probably some kind of gloda corruption. I'll restart Thunderbird and see if it reoccurs without resetting gloda but if anyone has ideas how it might have gotten stuck in that indexing loop feel free to fix :)", "author": "merikes.lists@gmail.com", "id": 12296492, "time": "2017-05-05T17:58:36Z"}, {"text": "Thanks for reporting. I think the standard answer is: Delete global-messages-db.sqlite.", "author": "jorgk@jorgk.com", "id": 12297577, "time": "2017-05-06T06:13:19Z"}, {"text": "bug 535516 is the only place I've seen mentioned \"Observed header that claims to be gloda indexed but that gloda has never heard of during compaction\"\n\nIf it's just sucking CPU but not actually impacting responsiveness I'd ignore it until we decide how to go about solving it.  And, I must wonder whether your case is a somewhat recent regression - on the assumption that if this had been happening previously that you would have noticed", "author": "vseerror@lehigh.edu", "id": 12370582, "time": "2017-06-04T13:18:22Z"}, {"text": "Andrew, what might we make of this gloda.index_msg warning?", "author": "vseerror@lehigh.edu", "id": 12386320, "time": "2017-06-09T14:54:35Z"}, {"text": "I've gone cold turkey on Thunderbird, apologies.", "author": "bugmail@asutherland.org", "id": 12386557, "time": "2017-06-09T15:53:42Z"}, {"text": "(In reply to Wayne Mery (:wsmwk, NI for questions) from comment #2)\n> And, I must wonder\n> whether your case is a somewhat recent regression - on the assumption that\n> if this had been happening previously that you would have noticed\n\nMaybe. Just ran into it again yesterday and deleted gloda db once more. It's noticeable when your otherwise silent laptop starts up the fan for more than a short while :)", "author": "merikes.lists@gmail.com", "id": 12424748, "time": "2017-06-23T11:46:34Z"}, {"text": "This bug is not just running up CPU.  It also prevents message filters from running.", "author": "alanc@tech-world.com", "id": 12527515, "time": "2017-08-02T00:34:06Z"}, {"text": "Just witnessed it happening for at least 4th time since reporting this bug. Disabling gloda for now, too annoying not to.", "author": "merikes.lists@gmail.com", "id": 12551786, "time": "2017-08-09T20:17:25Z"}, {"text": "59.0b1 seeing this bug now.  25% cpu, 100 MB memory, no disk.  Browser console seeing this message 1 per second.", "author": "dmccammishjr@gmail.com", "id": 13066989, "time": "2018-02-25T23:19:28Z"}, {"text": "2018-02-25 17:15:10\tgloda.index_msg\tWARN\tObserved header that claims to be gloda indexed but that gloda has never heard of during compaction. In folder: mailbox://nobody@Local%20Folders/DNA%20Project sketchy key: 13 subject: Fwd: FW: SNP report from Barry McCain\n2018-02-25 17:15:11\tgloda.index_msg\tWARN\tObserved header that claims to be gloda indexed but that gloda has never heard of during compaction. In folder: mailbox://nobody@Local%20Folders/DNA%20Project sketchy key: 13 subject: Fwd: FW: SNP report from Barry McCain\n\nMessage above referencing a Local Folder with property for retention (via account settings) dated set to \"Don't delete.\"  Message is dated 2015.  Not the oldest or newest message in the folder.", "author": "dmccammishjr@gmail.com", "id": 13067004, "time": "2018-02-25T23:36:30Z"}, {"text": "Just a thought: This may be related to messages moved or deleted from the inbox before gloda indexing occurs.  I'm using Filtaquilla to trash some types of spam.  The trash is not include in the global search.  POP3, shared inbox, local folders.\n\nGloda indexing should show zero/one error messages, ignore the entry and not continuously retry.", "author": "alanc@tech-world.com", "id": 13068358, "time": "2018-02-26T03:20:17Z"}, {"text": "This bug definitely an issue in 60.0b1.   No extensions enabled.  Very obvious in editor performance - serious delay in appearance of keystrokes because of the siphoned off processor cycles.", "author": "dmccammishjr@gmail.com", "id": 13171413, "time": "2018-04-01T19:52:53Z"}, {"text": "Continues to be an issue in 60.0b2.", "author": "dmccammishjr@gmail.com", "id": 13200647, "time": "2018-04-12T13:04:40Z"}, {"text": "(In reply to doug2 from comment #12)\n> Continues to be an issue in 60.0b2.\n\nWe need more useful data than that to make progress :)\n\nbug 1406653 was reported against 56 beta\nbug 1362483 was reported against 54 beta (this bug)\n\nSo, can anyone reproduce this with version 52, or 53?", "author": "vseerror@lehigh.edu", "id": 13202080, "time": "2018-04-12T20:26:03Z"}, {"text": "Tell me how to get those versions and I'll try to reproduce the problem with them.\nI read bug 1406653 and it sounds like a duplicate to me, but I may not be seeing the nuances. While watching the error console, the red numbers are not sequential and seem to be dynamic - i.e., somehow the number in red changes within a single error message.\n\nAfter reading 1406653, I was able to clear the Gloda error by \"repairing\" the Inbox folder. (Previously, I shutdown TB and restarted it.) I too have a bunch of Local folders, but they don't seem to be involved.  The problem seems to occur when mail is deleted, but that could just be the most common database action.", "author": "dmccammishjr@gmail.com", "id": 13202416, "time": "2018-04-12T22:19:38Z"}, {"text": "Prior versions at https://releases.mozilla.org/pub/thunderbird/releases/", "author": "vseerror@lehigh.edu", "id": 13202820, "time": "2018-04-13T02:28:52Z"}, {"text": "Ran 54.0b3 for a day, 56.0b4 (auto-updated twice) for a day or more - no observed Gloda issues (but reported crashes). Have downloaded 53.0b2 and will run for a few days.", "author": "dmccammishjr@gmail.com", "id": 13205829, "time": "2018-04-14T13:30:07Z"}, {"text": "53.0b2  Not seeing repeated Gloda error, but do see error for each received message.  Below is error log transcript.\nI will continue to monitor 53.0b2.\nIndexedDB UnknownErr: ActorsParent.cpp:594  (unknown)\nundefined  Promise-backend.js:920\n1523713830754\taddons.xpi-utils\tWARN\tCould not find source bundle for add-on {972ce4c6-7e08-4474-a285-3208198ce6fd}: [Exception... \"Component returned failure code: 0x80004005 (NS_ERROR_FAILURE) [nsIFile.persistentDescriptor]\"  nsresult: \"0x80004005 (NS_ERROR_FAILURE)\"  location: \"JS frame :: resource://gre/modules/addons/XPIProvider.jsm -> resource://gre/modules/addons/XPIProviderUtils.js :: parseDB :: line 629\"  data: no] Stack trace: parseDB()@resource://gre/modules/addons/XPIProvider.jsm -> resource://gre/modules/addons/XPIProviderUtils.js:629 < syncLoadDB()@resource://gre/modules/addons/XPIProvider.jsm -> resource://gre/modules/addons/XPIProviderUtils.js:563 < checkForChanges()@resource://gre/modules/addons/XPIProvider.jsm:3758 < startup()@resource://gre/modules/addons/XPIProvider.jsm:2793 < callProvider()@resource://gre/modules/AddonManager.jsm:272 < _startProvider()@resource://gre/modules/AddonManager.jsm:756 < startup()@resource://gre/modules/AddonManager.jsm:938 < startup()@resource://gre/modules/AddonManager.jsm:3129 < observe()@resource://gre/components/addonManager.js:65\nIndexedDB UnknownErr: ActorsParent.cpp:594  (unknown)\nUnknownError  indexed-db.js:58:9\nundefined  Promise-backend.js:920\nUnknownError  indexed-db.js:58:9\n2018-04-14 09:20:35\tgloda.datastore\tERROR\tgot error in _asyncTrackerListener.handleError(): 19: constraint failed\n  log4moz.js:690\n2018-04-14 09:20:35\tgloda.datastore\tERROR\tgot error in _asyncTrackerListener.handleError(): 19: constraint failed\n\nAsync statement execution returned with '1', 'no such table: moz_favicons' nsPlacesExpiration.js:691\n2018-04-14 10:30:34\tgloda.datastore\tERROR\tgot error in _asyncTrackerListener.handleError(): 19: constraint failed\n  log4moz.js:690\n2018-04-14 10:30:34\tgloda.datastore\tERROR\tgot error in _asyncTrackerListener.handleError(): 19: constraint failed\n\nAsync statement execution returned with '1', 'no such table: moz_favicons'  nsPlacesExpiration.js:691\nAsync statement execution returned with '1', 'no such table: moz_favicons' nsPlacesExpiration.js:691\n2018-04-14 12:11:04\tgloda.datastore\tERROR\tgot error in _asyncTrackerListener.handleError(): 19: constraint failed\n  log4moz.js:690\n2018-04-14 12:11:04\tgloda.datastore\tERROR\tgot error in _asyncTrackerListener.handleError(): 19: constraint failed\n\nAsync statement execution returned with '1', 'no such table: moz_favicons' nsPlacesExpiration.js:691\n2018-04-14 12:41:34\tgloda.datastore\tERROR\tgot error in _asyncTrackerListener.handleError(): 19: constraint failed\n  log4moz.js:690\n2018-04-14 12:41:34\tgloda.datastore\tERROR\tgot error in _asyncTrackerListener.handleError(): 19: constraint failed", "author": "dmccammishjr@gmail.com", "id": 13205950, "time": "2018-04-14T17:48:50Z"}, {"text": "Not seeing the Gloda problem in 53.0b2.  Going to 56.0b4.", "author": "dmccammishjr@gmail.com", "id": 13211214, "time": "2018-04-16T23:46:11Z"}, {"text": "gloda problem occurred this am in 56.0b4 \"spontaneously\"  (was not using TB at the time, may have been compacting or receiving)\nRepairing inbox stopped the gloda error repeats.  !! Cancel that.!!  Gloda error repeats restarted shortly after \"Repair.\"  Tried again and Gloda error repeats again immediately restarted.   Had to shut down TB.  MS Superfetch went wild for a while.  Very high disk use.", "author": "dmccammishjr@gmail.com", "id": 13212346, "time": "2018-04-17T13:30:13Z"}, {"text": "Is this problem Windows-specific?  For other performance reasons, I shut down the \"Superfetch\" process while running TB56.0b4 and have not had further problems. Could there be some sort of timing conflict?  Now back to 60.0b3.  Will continue to monitor.", "author": "dmccammishjr@gmail.com", "id": 13218651, "time": "2018-04-19T14:34:26Z"}, {"text": "(In reply to doug2 from comment #20)\n> Is this problem Windows-specific?\nNo, the initial report is on linux.", "author": "merikes.lists@gmail.com", "id": 13219107, "time": "2018-04-19T17:08:28Z"}, {"text": "Gloda problem exists on 60.0b5.", "author": "dmccammishjr@gmail.com", "id": 13249892, "time": "2018-05-02T18:22:53Z"}, {"text": ">> We need more useful data than that to make progress :)\n\nNo, you really don't.  We don't need the underlying cause fixed.  We need the endless retry and logging of an unknown object fixed.  If the gloda scan would just skip missing entries and move on, it wouldn't be hogging CPU.", "author": "alanc@tech-world.com", "id": 13253732, "time": "2018-05-04T01:15:48Z"}, {"text": "(In reply to doug2 from comment #22)\n> Gloda problem exists on 60.0b5.\n\nclearly :)\n\ndoug2, thanks for your effort and progress trying to get the regression range.  So in summary:\n* 53.0b2 you don't see the problem\n* 54.0b3 you don't see the problem\n* 56.0b4 you see the problem\n* 55.0bx you don't mention \n\nIs that correct?\n\nThat should be sufficient for you further narrow down the regression range using https://mozilla.github.io/mozregression/quickstart.html  I would start with \"good\" date 2017-03-01 and \"bad\" date 2017-06-01  (which is 55.0a1 so I'm sort of guessing)", "author": "vseerror@lehigh.edu", "id": 13256548, "time": "2018-05-05T15:51:55Z"}, {"text": "(In reply to A Capesius from comment #23)\n> >> We need more useful data than that to make progress :)\n>\n> No, you really don't.  We don't need the underlying cause fixed. \n\ntrue, having a developer examine some hundreds or thousands of lines of code is one approach. However it is generally not successful or attractive.  Depends how long you want to wait for results - wait for one of 5-6 developers capable of debugging it to have time to do so, if more critical bugs aren't on their plate (which might be months or years), vs utilizing dozens or hundreds of users who with a little elbow grease and some guidance can often narrow the problem (which can take an hour or less).\n\nSo we tend to solve regressions indirectly, not from code examination by developers (who typically cannot reproduce the issue), but by leveraging available user manpower (who can easily reproduce) to get a one day regression range of code changes. We then can typically identify just a few lines of code that changed which are causing the problem.", "author": "vseerror@lehigh.edu", "id": 13256558, "time": "2018-05-05T16:03:22Z"}, {"text": "Correct.  Don't think I tested 55.0bx.  Note that problem did occur in 60.0bx which i'm guessing is an off branch.  \nI will work the regression range per above dates 2017-03-01 to 2017-06-01.", "author": "dmccammishjr@gmail.com", "id": 13256559, "time": "2018-05-05T16:06:16Z"}, {"text": "Ran bisection via mozregression from 2017-02-01 to 2017-06-01 and did not encounter the Gloda issue. Actually, the last daily tested was 2017-05-11 (I think) [i.e., mozregression did not find a daily past 2017-05-11?].  Also of note, I did not encounter the Gloda issue in my parallel 56.0bx install reading/writing daily \"real\" mail.  During the bisection, I tried to load the daily with mail, but fewer than 50 pieces. I moved a number of messages to trash, compacted the Inbox, etc.  If someone can give guidance on using mozregression with a larger dataset, I will run it again.  I want to take a copy of my \"real,\" very large dataset and run the dailies against it. Does this require using the command line version?  The only active mail would be one test address, but I could pre-populate the Inbox, Trash, etc.?  Need guidance on setting that up. (Stop auto retrieve of mail for all id's except the test one, etc.)", "author": "dmccammishjr@gmail.com", "id": 13265218, "time": "2018-05-09T12:53:17Z"}, {"text": "Merike is this highly reproducible for you?", "author": "vseerror@lehigh.edu", "id": 13265524, "time": "2018-05-09T14:49:44Z"}, {"text": "No, at least not recently. According to timestamps last time may have been on March 23rd as I delete gloda when it bothers me. At the time of reporting it was more like once a month. So if anything it's happening less often than it used to and definitely not often enough to narrow the range :(", "author": "merikes.lists@gmail.com", "id": 13266682, "time": "2018-05-09T20:56:05Z"}, {"text": ">> we tend to solve regressions indirectly, not from code examination by developers (who typically cannot reproduce the issue), but by leveraging available user manpower (who can easily reproduce) to get a one day regression range of code changes. We then can typically identify just a few lines of code that changed which are causing the problem.\n\nPerhaps I wasn't clear enough.  My point was that there are two bugs here:  whatever is causing gloda to have moved/deleted items indexed, and the compactor routine going into an endless loop when it can't find an item.  Fixing the former is not going to fix the latter, it will just obscure it until next time.  And the looping problem is what is causing our issue.  This looping bug could go back to original code for all I know and is just now exposed by some other indexing issue.  \n\nWhile the item going missing could be happening anywhere, the looping bug is clearly localized around the error message it generates, so 10000 lines of code don't need to be searched.   comm-central/mailnews/db/gloda/modules/index_msg.js\n Line 1292.  Looks to me that case 2 should delete the gloda entry like case 1b, but I'm no expert on the internals.  The goal would be to prevent thousands of messages from being generated instead of one.", "author": "alanc@tech-world.com", "id": 13267108, "time": "2018-05-10T00:20:22Z"}, {"text": "I understood your earlier point.  The code in question isn't at fault unless it changed, which you can determine yourself using annotated mode.  And if it didn't change then you are back to my point of finding the regression range.", "author": "vseerror@lehigh.edu", "id": 13267234, "time": "2018-05-10T02:14:04Z"}, {"text": "Condition dependent bugs don't always manifest up front.  That code section handles other cases of indexing issues, it just doesn't handle this particular one correctly.  It really is two separate issues.", "author": "alanc@tech-world.com", "id": 13267278, "time": "2018-05-10T03:02:32Z"}, {"text": "I wish there were a better way to discuss these side issues. I have a question about TBird user population. Is this a POP-only issue? Is POP a very minor part of TBird use?  I switched back to POP many years ago because MAPI was terrible about messages with large attachments. Bottom line - are we worrying about a problem that only affects 1% of TBird users?  I've been using TBird a LONG time and don't want to switch.  Is TBird losing users (as well as developers)?\n#2 As soon as I can, I will figure out how to use the command line mozregression and build a large test dataset and run another regression with it.  My last Gloda issue occurrence is noted in comment 22 - 8 days ago. No change in software or the size of my dataset.  Use of TBird about the same. Did not delete and rebuild the index. No idea at this point why issue \"went away.\"", "author": "dmccammishjr@gmail.com", "id": 13267875, "time": "2018-05-10T11:35:21Z"}, {"text": "Is this issue pop only, I don't know. The other protocol is IMAP (not mapi).\n\nIf this only affects 1% of users, then it affects 250,000 users  (estimated user base is ~25 million)   (FWIW over the last several years TB is gaining users. And in the past year developers increased significantly - but I don't see what that has to do with this bug)\n\nSide issues which are unrelated to this bug ... see https://www.thunderbird.net/en-US/get-involved/#communication\n- support questions should be posted in SUMO https://support.mozilla.org/en-US/questions/thunderbird\n- general thunderbird questions https://discourse.mozilla.org/c/thunderbird\n\nThanks for continuing to look into mozregression.", "author": "vseerror@lehigh.edu", "id": 13268364, "time": "2018-05-10T15:14:13Z"}, {"text": "That's great news. I was concerned we were working on a bug nobody cared about. I will work the mozregression.", "author": "dmccammishjr@gmail.com", "id": 13269424, "time": "2018-05-10T21:33:54Z"}, {"text": "Do you have saved search folders?", "author": "vseerror@lehigh.edu", "id": 13362808, "time": "2018-05-25T03:21:51Z"}, {"text": "(In reply to Wayne Mery (:wsmwk) from comment #36)\n> Do you have saved search folders?\n\nNo", "author": "alanc@tech-world.com", "id": 13362882, "time": "2018-05-25T04:52:41Z"}, {"text": "No", "author": "dmccammishjr@gmail.com", "id": 13363630, "time": "2018-05-25T12:21:25Z"}], "version": "54", "qa_contact": "", "see_also": ["https://bugzilla.mozilla.org/show_bug.cgi?id=1349915", "https://bugzilla.mozilla.org/show_bug.cgi?id=1406653"], "component": "Database", "cf_status_firefox62": "---", "cf_status_firefox61": "---", "cf_status_firefox60": "---", "cf_qa_whiteboard": "", "whiteboard": "", "op_sys": "Unspecified", "groups": [], "classification": "Components", "cc": ["alanc@tech-world.com", "dmccammishjr@gmail.com", "jorgk@jorgk.com", "mcastelluccio@mozilla.com", "nigel@nigelbrooks.com", "vseerror@lehigh.edu"], "cf_status_firefox_esr60": "---", "cf_user_story": "", "cf_status_thunderbird_53": "---", "creator": "merikes.lists@gmail.com", "priority": "--", "comment_count": 39, "cf_tracking_firefox_esr52": "---", "cf_fx_iteration": "---", "cf_tracking_thunderbird_59": "---", "cf_tracking_thunderbird_58": "---", "cf_tracking_thunderbird_57": "---", "cf_tracking_thunderbird_56": "---", "cf_tracking_thunderbird_54": "---", "cf_tracking_thunderbird_53": "---", "cf_tracking_thunderbird_52": "---", "is_cc_accessible": true, "cc_detail": [{"email": "alanc@tech-world.com", "id": 332365, "name": "alanc@tech-world.com", "real_name": "A Capesius"}, {"email": "dmccammishjr@gmail.com", "id": 386898, "name": "dmccammishjr@gmail.com", "real_name": "doug2"}, {"email": "jorgk@jorgk.com", "id": 176914, "name": "jorgk@jorgk.com", "real_name": "Jorg K (GMT+2)"}, {"email": "mcastelluccio@mozilla.com", "id": 420453, "name": "mcastelluccio@mozilla.com", "real_name": "Marco Castelluccio [:marco]"}, {"email": "nigel@nigelbrooks.com", "id": 595912, "name": "nigel@nigelbrooks.com", "real_name": ""}, {"email": "vseerror@lehigh.edu", "id": 29811, "name": "vseerror@lehigh.edu", "real_name": "Wayne Mery (:wsmwk)"}], "dupe_of": null, "cf_fx_points": "---", "history": [{"changes": [{"removed": "", "field_name": "cc", "added": "jorgk@jorgk.com"}], "who": "jorgk@jorgk.com", "when": "2017-05-06T06:13:19Z"}, {"changes": [{"removed": "", "field_name": "keywords", "added": "perf"}, {"removed": "", "field_name": "cc", "added": "vseerror@lehigh.edu"}], "who": "vseerror@lehigh.edu", "when": "2017-06-04T13:18:22Z"}, {"changes": [{"removed": "", "field_name": "cc", "added": "bugmail@asutherland.org"}, {"removed": "Gloda stuck in a loop with \"Observed header that claims to be gloda indexed but that gloda has never heard of during compaction\"", "field_name": "summary", "added": "Gloda stuck in a loop with high CPU and error console gloda.index_msg WARN \"Observed header that claims to be gloda indexed but that gloda has never heard of during compaction\". Folder repair moves the issue to a different folder."}, {"removed": "", "field_name": "flagtypes.name", "added": "needinfo?(bugmail@asutherland.org)"}], "who": "vseerror@lehigh.edu", "when": "2017-06-09T14:54:35Z"}, {"changes": [{"removed": "", "field_name": "see_also", "added": "https://bugzilla.mozilla.org/show_bug.cgi?id=1349915"}], "who": "vseerror@lehigh.edu", "when": "2017-06-09T15:12:12Z"}, {"changes": [{"removed": "bugmail@asutherland.org", "field_name": "cc", "added": ""}, {"removed": "needinfo?(bugmail@asutherland.org)", "field_name": "flagtypes.name", "added": ""}], "who": "bugmail@asutherland.org", "when": "2017-06-09T15:53:42Z"}, {"changes": [{"removed": "", "field_name": "cc", "added": "alanc@tech-world.com"}], "who": "alanc@tech-world.com", "when": "2017-08-02T00:34:06Z"}, {"changes": [{"removed": "", "field_name": "see_also", "added": "https://bugzilla.mozilla.org/show_bug.cgi?id=1406653"}], "who": "vseerror@lehigh.edu", "when": "2018-01-07T17:45:01Z"}, {"changes": [{"removed": "", "field_name": "cc", "added": "dmccammishjr@gmail.com"}], "who": "dmccammishjr@gmail.com", "when": "2018-02-25T23:17:50Z"}, {"changes": [{"removed": "", "field_name": "flagtypes.name", "added": "needinfo?(dmccammishjr@gmail.com)"}], "who": "vseerror@lehigh.edu", "when": "2018-04-12T20:26:03Z"}, {"changes": [{"removed": "needinfo?(dmccammishjr@gmail.com)", "field_name": "flagtypes.name", "added": ""}], "who": "dmccammishjr@gmail.com", "when": "2018-04-14T13:30:07Z"}, {"changes": [{"removed": "", "field_name": "flagtypes.name", "added": "needinfo?(dmccammishjr@gmail.com)"}], "who": "vseerror@lehigh.edu", "when": "2018-05-05T15:51:55Z"}, {"changes": [{"removed": "", "field_name": "keywords", "added": "regression, regressionwindow-wanted"}, {"removed": "", "field_name": "cc", "added": "mcastelluccio@mozilla.com"}], "who": "mcastelluccio@mozilla.com", "when": "2018-05-07T18:34:19Z"}, {"changes": [{"removed": "", "field_name": "cc", "added": "nigel@nigelbrooks.com"}, {"removed": "", "field_name": "flagtypes.name", "added": "needinfo?(merikes.lists@gmail.com)"}], "who": "vseerror@lehigh.edu", "when": "2018-05-09T14:49:44Z"}, {"changes": [{"removed": "needinfo?(merikes.lists@gmail.com)", "field_name": "flagtypes.name", "added": ""}], "who": "merikes.lists@gmail.com", "when": "2018-05-09T20:56:05Z"}, {"changes": [{"removed": "", "field_name": "flagtypes.name", "added": "needinfo?(alanc@tech-world.com)"}], "who": "vseerror@lehigh.edu", "when": "2018-05-25T03:21:51Z"}, {"changes": [{"removed": "needinfo?(alanc@tech-world.com)", "field_name": "flagtypes.name", "added": ""}], "who": "alanc@tech-world.com", "when": "2018-05-25T04:52:41Z"}, {"changes": [{"removed": "needinfo?(dmccammishjr@gmail.com)", "field_name": "flagtypes.name", "added": ""}], "who": "dmccammishjr@gmail.com", "when": "2018-05-25T12:21:25Z"}], "id": 1362483, "severity": "normal", "status": "NEW", "product": "MailNews Core", "cf_status_firefox_esr52": "---", "blocks": [], "cf_tracking_seamonkey249": "---", "target_milestone": "---", "url": "", "resolution": "", "creator_detail": {"email": "merikes.lists@gmail.com", "id": 310218, "name": "merikes.lists@gmail.com", "real_name": "Merike Sell (:merike)"}, "mentors": [], "last_change_time": "2018-05-25T12:21:25Z", "alias": null, "flags": [], "assigned_to": "nobody@mozilla.org", "cf_status_thunderbird_esr60": "---", "cf_status_thunderbird_62": "---", "cf_status_thunderbird_61": "---", "cf_status_thunderbird_60": "---"}