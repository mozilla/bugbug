# %%
%pip install -q bugbug==0.0.570

# %%
from bugbug import utils
from bugbug.code_search.mozilla import FunctionSearchMozilla


def get_file(commit_hash, path):
    r = utils.get_session("hgmo").get(
        f"https://hg.mozilla.org/mozilla-unified/raw-file/{commit_hash}/{path}",
        headers={
            "User-Agent": utils.get_user_agent(),
        },
    )
    r.raise_for_status()
    return r.text


repo_dir = "../mozilla-unified"
function_search = FunctionSearchMozilla(repo_dir, get_file, True)


# %%
from dotenv import load_dotenv

from bugbug.generative_model_tool import create_openai_llm

load_dotenv("~/.env")

llm = create_openai_llm()

# %%
from bugbug.tools import code_review
from bugbug.vectordb import QdrantVectorDB

review_comments_db = code_review.ReviewCommentsDB(QdrantVectorDB("diff_comments"))


suggestions_feedback_db = code_review.SuggestionsFeedbackDB(
    QdrantVectorDB("suggestions_feedback")
)

review_platform = "phabricator"
review_data: code_review.ReviewData = code_review.review_data_classes[review_platform]()


# %%
tool = code_review.CodeReviewTool(
    llm=llm,
    # function_search=function_search,
    review_comments_db=review_comments_db,
    suggestions_feedback_db=suggestions_feedback_db,
    verbose=False,
)


# %%
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

partial_variables = {
    "target_code_consistency": "Mozilla Firefox",
}

current_filtering_chain = LLMChain(
    prompt=PromptTemplate.from_template(
        code_review.PROMPT_TEMPLATE_FILTERING_ANALYSIS,
        partial_variables=partial_variables,
    ),
    llm=llm,
)


# %%
from scripts.code_review_tool_evaluator import (
    FeedbackEvaluator,
)

evaluator = FeedbackEvaluator("../bugbug_evaluation_datasets/evaluated_comments.csv")


# %%
import pandas as pd


def get_latest_evaluation_results_file():
    import glob

    files = glob.glob("evaluation_results_*#*.csv")
    if not files:
        raise FileNotFoundError(
            "No evaluation results file found. The file name should have `#` followed by a PR number."
        )

    return max(files)


# SELECTION (Option 1: same as before)
selected_review_requests = [
    (revision_id, code_review.ReviewRequest(diff_id))
    for revision_id, diff_id in pd.read_csv(get_latest_evaluation_results_file())[
        ["revision_id", "diff_id"]
    ]
    .drop_duplicates()
    .itertuples(name=None, index=False)
]

# SELECTION (Option 2: random sample)
# selected_review_requests = [
#     (revision_id, code_review.ReviewRequest(diff_id))
#     for revision_id, diff_id in evaluator.evaluated_comments
#     # .query("evaluation == 'CORRECT'")
#     [["revision_id", "diff_id"]].drop_duplicates().sample(100).itertuples(index=False)
# ]


# %%


def common_step():
    for review_request_id, review_request in selected_review_requests:
        print("---------------------------------------------------------")
        print(f"Review Request ID: {review_request_id}")
        print(f"Patch ID: {review_request.patch_id}")
        patch = review_data.get_patch_by_id(review_request.patch_id)
        print("---------------------------------------------------------")

        try:
            output = tool._generate_suggestions(patch)
        except code_review.FileNotInPatchError as e:
            print("Error while running the tool:", e)
            continue
        except code_review.LargeDiffError:
            print("Skipping the patch because it is too large.")
            continue

        unfiltered_suggestions = code_review.parse_model_output(output)
        if not unfiltered_suggestions:
            print("No suggestions were generated")
            continue

        rejected_examples = (
            "\n    - ".join(tool.get_similar_rejected_comments(unfiltered_suggestions))
            if tool.suggestions_feedback_db
            else code_review.DEFAULT_REJECTED_EXAMPLES
        )

        raw_output = current_filtering_chain.invoke(
            {
                "review": output,
                "patch": patch.raw_diff,
                "rejected_examples": rejected_examples,
            },
            return_only_outputs=True,
        )["text"]

        try:
            comments = list(
                code_review.generate_processed_output(raw_output, patch.patch_set)
            )
        except code_review.FileNotInPatchError as e:
            print("Error while running the tool:", e)
            continue

        # print_prettified_comments(comments)
        evaluation = evaluator.evaluate_diff_comments(review_request.patch_id, comments)
        evaluation_for_original = [
            {
                "variant_name": "Current Filtering",
                "revision_id": review_request_id,
                "diff_id": review_request.patch_id,
                **row,
            }
            for row in evaluation
        ]

        yield (
            rejected_examples,
            output,
            patch,
            review_request_id,
            review_request,
            evaluation_for_original,
        )


common_step_results = list(common_step())

# %%
import pickle

if "common_step_results" not in locals():
    raise ValueError(
        "common_step_results is not defined. Please run the common_step() function first."
    )


with open("common_step_results.pkl", "wb") as f:
    pickle.dump(common_step_results, f)

# %%
import pickle

with open("common_step_results.pkl", "rb") as f:
    common_step_results = pickle.load(f)

# %%
from datetime import datetime

NEW__PROMPT_TEMPLATE_FILTERING_ANALYSIS = """Filter review comments to keep those that:
- are consistent with the {target_code_consistency} source code;
- focus on reporting possible bugs, functional regressions, issues, or similar concerns;
- report readability or design concerns.

Exclude comments that:
- only describe the change;
- restate obvious facts like renamed variables or replaced code;
- include praising;
- ask if changes are intentional or ask to ensure things exist.

Do not report any explanation about your choice. Only return a valid JSON list.

Comments:
{comments}

As examples of not expected comments, not related to the current patch, please, check some below:
    - {rejected_examples}
"""

new_filtering_chain = LLMChain(
    prompt=PromptTemplate.from_template(
        NEW__PROMPT_TEMPLATE_FILTERING_ANALYSIS,
        partial_variables=partial_variables,
    ),
    llm=llm,
)


all_variants_evaluation_results = []

for (
    rejected_examples,
    output,
    patch,
    review_request_id,
    review_request,
    evaluation_for_original,
) in common_step_results:

    all_variants_evaluation_results.extend(evaluation_for_original)

    raw_output = new_filtering_chain.invoke(
        {
            "comments": output,
            "rejected_examples": rejected_examples,
        },
        return_only_outputs=True,
    )["text"]

    comments = list(code_review.generate_processed_output(raw_output, patch.patch_set))

    # print_prettified_comments(comments)
    evaluation = evaluator.evaluate_diff_comments(review_request.patch_id, comments)
    all_variants_evaluation_results.extend(
        {
            "variant_name": "New Filtering",
            "revision_id": review_request_id,
            "diff_id": review_request.patch_id,
            **row,
        }
        for row in evaluation
    )

    evaluation_result_all_columns = [
        "variant_name",
        "revision_id",
        "diff_id",
        "new_comment",
        "old_comments_count",
        "matched",
        "old_comment",
        "evaluation",
    ]


evaluation_results_file = (
    f"evaluation_results_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv"
)


df = pd.DataFrame(
    all_variants_evaluation_results, columns=evaluation_result_all_columns
)
df.to_csv(
    evaluation_results_file,
    index=False,
    header=True,
    mode="w",
)


# %%
print("Done!")
# %%
